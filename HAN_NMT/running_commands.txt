
1) Training the sentence-level NMT baseline:

python train.py -data /data/dominika/git/HAN_NMT/preprocess_TED_zh-en -save_model sentence_level_model -encoder_type transformer -decoder_type transformer -enc_layers 6 -dec_layers 6 -label_smoothing 0.1 -src_word_vec_size 512 -tgt_word_vec_size 512 -rnn_size 512 -position_encoding -dropout 0.1 -batch_size 4096 -start_decay_at 20 -report_every 500 -epochs 20 -gpuid 0 -max_generator_batches 16 -batch_type tokens -normalization tokens -accum_count 4 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot -train_part sentences

2) Training HAN-encoder using the sentence-level NMT model:

python train.py -data /data/dominika/git/HAN_NMT/preprocess_TED_zh-en -save_model HAN_enc_model -encoder_type transformer -decoder_type transformer -enc_layers 6 -dec_layers 6 -label_smoothing 0.1 -src_word_vec_size 512 -tgt_word_vec_size 512 -rnn_size 512 -position_encoding -dropout 0.1 -batch_size 1024 -start_decay_at 2 -report_every 500 -epochs 1 -gpuid 0 -max_generator_batches 32 -batch_type tokens -normalization tokens -accum_count 4 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot -train_part all -context_type HAN_enc -context_size 3 -train_from sentence_level_model

3) Training HAN-decoder using the sentence-level NMT model:

python train.py -data /data/dominika/git/HAN_NMT/preprocess_TED_zh-en -save_model HAN_dec_model -encoder_type transformer -decoder_type transformer -enc_layers 6 -dec_layers 6 -label_smoothing 0.1 -src_word_vec_size 512 -tgt_word_vec_size 512 -rnn_size 512 -position_encoding -dropout 0.1 -batch_size 1024 -start_decay_at 2 -report_every 500 -epochs 1 -gpuid 0 -max_generator_batches 32 -batch_type tokens -normalization tokens -accum_count 4 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot -train_part all -context_type HAN_dec -context_size 3 -train_from sentence_level_model

4) Training HAN-joint using the HAN-encoder model:

python train.py -data /data/dominika/git/HAN_NMT/preprocess_TED_zh-en -save_model HAN_joint_model -encoder_type transformer -decoder_type transformer -enc_layers 6 -dec_layers 6 -label_smoothing 0.1 -src_word_vec_size 512 -tgt_word_vec_size 512 -rnn_size 512 -position_encoding -dropout 0.1 -batch_size 1024 -start_decay_at 2 -report_every 500 -epochs 1 -gpuid 0 -max_generator_batches 32 -batch_type tokens -normalization tokens -accum_count 4 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot -train_part all -context_type HAN_join -context_size 3 -train_from HAN_enc_model




5) Translate

python translate.py -model HAN_joint_model_acc_41.64_ppl_25.45_e1.pt -src ../preprocess_TED_zh-en/IWSLT15.TED.tst2013.tc.zh -doc ../preprocess_TED_zh-en/IWSLT15.TED.tst2013.zh-en.doc -output out_zh_en_translations -translate_part all -batch_size 1000 -gpu 0

